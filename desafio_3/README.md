# Tokenization by word and char

- Seleccionar un corpus de texto sobre el cual entrenar el modelo de lenguaje.
- Realizar el pre-procesamiento adecuado para tokenizar el corpus, estructurar el dataset y separar entre datos de entrenamiento y validación.
- Proponer arquitecturas de redes neuronales basadas en unidades recurrentes para implementar un modelo de lenguaje.
- Con el o los modelos que consideren adecuados, generar nuevas secuencias a partir de secuencias de contexto con las estrategias de greedy search y beam search determístico y estocástico. En este último caso observar el efecto de la temperatura en la generación de secuencias.

### Instalación:
`python version: ^3.10`

`pip install -r requirements.txt`

### Corpus de texto utilizado: 
[Orgullo y Prejuicio - Jane Austen](https://www.textos.info/jane-austen/orgullo-y-prejuicio)

### Colab
[Tokenización por palabra](https://colab.research.google.com/drive/1LkWok653p_4QlKI_Vp4QyFv6_X_8fDnz?usp=sharing)
[Tokenización por caracter](https://colab.research.google.com/drive/1v4nmvaJFaCdGGgVL-VmF37ALu9KC36-F?usp=sharing)